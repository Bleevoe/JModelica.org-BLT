\documentclass[]{beamer}
\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{dtsyntax}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{float}
\usepackage{ulem}
\usepackage{color}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{epstopdf}
\usepackage{bm}
\newcommand{\cond}{\mathbf{cond}}

\newcommand\gsout{\bgroup\markoverwith
{\textcolor{green}{\rule[3.1pt]{2pt}{1pt}}}\ULon}

%%%%%%%%%% Regler Beamer themes %%%%%%%%%%%%%%%%%
%\usepackage[lionbackground]{beamerthemeRegler}
%\usepackage[lioncorner]{beamerthemeRegler}
%\usepackage[lionheader]{beamerthemeRegler}
\usetheme[lionheader]{Regler}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Title page
\title{Implicit vs. explicit ODE}
\author{Fredrik Magnusson\inst{1} \and Karl Berntorp\inst{2}}
\institute
{
\inst{1} Department of Automatic Control \\
Lund University, Sweden \\
\vspace{14pt}
\inst{2} Mitsubishi Electric Research Laboratories \\
Cambridge, MA \\
\vspace{14pt}
\insertdate
}
\date{November 11, 2015}


% Slide numbering
\definecolor{FootGrey}{RGB}{83,121,170}
\setbeamercolor{foot}{fg=FootGrey,bg=white}
\setbeamertemplate{footline}{
    \begin{beamercolorbox}[right, sep=2.5pt]{foot}
        \insertframenumber{} / \inserttotalframenumber
    \end{beamercolorbox}
}

\begin{document}

{
\setbeamertemplate{footline}{}
\begin{frame}[noframenumbering]
    \titlepage
\end{frame}
}

\begin{frame}
\frametitle{Review}
Last time:
\begin{itemize}
\item
Problem behaves like high-index problem (sup. alg.)
\item
Maybe Pacejka's Magic formula is almost high index?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preview}
This time:
\begin{itemize}
\item
Revisit simulation results, looking at order and step length
\item
Discuss ``almost high index''
\begin{itemize}
\item
Results indicate that Magic formula is harmless
\item
Results indicate that problem is not almost high index
\end{itemize}
\end{itemize}
If time permits:
\begin{itemize}
\item
Discuss optimization formulation and solution procedure
\item
Discuss if the observed simulation phenomena regarding DAE vs. ODE are related to optimization
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Simulation results}
Radau5 tolerance chosen to get reference solution, the rest to get similar accuracy

{\small
\begin{table}
\centering
\begin{tabular}{lccc}
\toprule
Setup & tol & Time [s] & Steps [1000] \\
\midrule
Radau5 DAE & 1e-12 & 69 & 17 \\
IDA DAE & 1e-6 & 59 & 26 \\
IDA DAE sup. alg. & 1e-8 & 2.8 & 2.9 \\
IDA DAE par. & 1e-6 & 6.3 & 8 \\
IDA DAE par. sup. & 1e-8 & 1.6 & 3.2 \\
IDA ODE & 1e-8 & 0.9 & 3.4 \\
\bottomrule
\end{tabular}
\end{table}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{Radau5 DAE}
{\small
\begin{verbatim}
Final Run Statistics: --- 

 Number of steps                                 : 16701
 Number of function evaluations                  : 231074
 Number of Jacobian evaluations                  : 12338
 Number of function eval. due to Jacobian eval.  : 407154
 Number of error test failures                   : 1040
 Number of LU decompositions                     : 21266

Solver options:

 Solver                  : Radau5 (implicit)
 Tolerances (absolute)   : [  1.00000000e-12]
 Tolerances (relative)   : 1e-12
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{IDA DAE}
{\small
\begin{verbatim}
Final Run Statistics: --- 

 Number of steps                                 : 26361
 Number of function evaluations                  : 67823
 Number of Jacobian evaluations                  : 25991
 Number of function eval. due to Jacobian eval.  : 857703
 Number of error test failures                   : 12046
 Number of nonlinear iterations                  : 67823
 Number of nonlinear convergence failures        : 0

Solver options:

 Solver                       : IDA (BDF)
 Maximal order                : 5
 Suppressed algebr. variables : False
 Tolerances (absolute)        : 1e-06
 Tolerances (relative)        : 1e-06
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{IDA DAE sup. alg.}
{\small
\begin{verbatim}
Final Run Statistics: --- 

 Number of steps                                 : 2923
 Number of function evaluations                  : 7072
 Number of Jacobian evaluations                  : 1065
 Number of function eval. due to Jacobian eval.  : 35145
 Number of error test failures                   : 424
 Number of nonlinear iterations                  : 7072
 Number of nonlinear convergence failures        : 0

Solver options:

 Solver                       : IDA (BDF)
 Maximal order                : 5
 Suppressed algebr. variables : True
 Tolerances (absolute)        : 1e-08
 Tolerances (relative)        : 1e-08
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{IDA DAE par.}
{\small
\begin{verbatim}
Final Run Statistics: --- 

 Number of steps                                 : 8489
 Number of function evaluations                  : 18467
 Number of Jacobian evaluations                  : 5952
 Number of function eval. due to Jacobian eval.  : 83328
 Number of error test failures                   : 2867
 Number of nonlinear iterations                  : 18467
 Number of nonlinear convergence failures        : 0

Solver options:

 Solver                       : IDA (BDF)
 Maximal order                : 5
 Suppressed algebr. variables : False
 Tolerances (absolute)        : 1e-06
 Tolerances (relative)        : 1e-06
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{IDA DAE par. sup.}
{\small
\begin{verbatim}
Final Run Statistics: --- 

 Number of steps                                 : 3240
 Number of function evaluations                  : 7617
 Number of Jacobian evaluations                  : 1120
 Number of function eval. due to Jacobian eval.  : 15680
 Number of error test failures                   : 476
 Number of nonlinear iterations                  : 7617
 Number of nonlinear convergence failures        : 0

Solver options:

 Solver                       : IDA (BDF)
 Maximal order                : 5
 Suppressed algebr. variables : True
 Tolerances (absolute)        : 1e-08
 Tolerances (relative)        : 1e-08
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{IDA ODE}
{\small
\begin{verbatim}
Final Run Statistics: --- 

 Number of steps                                 : 3428
 Number of function evaluations                  : 5524
 Number of Jacobian evaluations                  : 919
 Number of function eval. due to Jacobian eval.  : 9190
 Number of error test failures                   : 477
 Number of nonlinear iterations                  : 5524
 Number of nonlinear convergence failures        : 0

Solver options:

 Solver                       : IDA (BDF)
 Maximal order                : 5
 Suppressed algebr. variables : False
 Tolerances (absolute)        : 1e-08
 Tolerances (relative)        : 1e-08
\end{verbatim}
}
\end{frame}

\begin{frame}
\frametitle{Steps DAE}
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{steps_dae.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Steps ODE}
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{steps_ode.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Index}
\begin{itemize}
\item
Key seems to be to deactivate error control for (some) algebraics
\item
Old conjecture: The problem is almost high index
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Almost high index}
This is my understanding of almost high index:
\begin{align}
\dot x &= x + y \\
y &= ax
\end{align}
This is index 1 for all $\alpha \in \mathbb{R}$, but as $a \rightarrow \infty$, it becomes index 2.
\end{frame}

\begin{frame}
\frametitle{Almost high index}
\begin{itemize}
\item
The change of variables $z = ax$ yields
\begin{align}
\dot z &= z + ay \\
y &= z
\end{align}
which is index 1 even as $\alpha \rightarrow \infty$
\item
So is ``almost high index'' just a result of poor scaling?
\item
Also, it seems like transforming an almost high index DAE to an ODE will just result in a very stiff ODE
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Magic formula}
	\begin{align}
		F^x_{0} &= \mu_x F^z 
		\sin\Big( C_{x} 
		\arctan\big(B_{x} \lambda_i -E_{x}(B_{x}\lambda-\arctan B_{x}\lambda)\big)\Big)
	\end{align}
	\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{magic_formula.pdf}
	\end{figure}
\end{frame}

\begin{frame}
\frametitle{Magic formula}
During simulation, $\lambda$ stays below 0.05
\begin{figure}[H]
	\centering
	\includegraphics[width=0.67\linewidth]{force_grad.png}
\end{figure}
Almost linear, with very high eigenvalues (can be remedied by variable scaling)
\end{frame}

\begin{frame}
\frametitle{Magic formula}
Changes in gradient not cause of trouble:
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{force_grad_steps.png}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Measuring index}
\begin{itemize}
\item
How to measure distance to index 2? Two ideas:
\begin{itemize}
\item
Compute condition number of iteration matrix in discretization method and see how it varies with $h$
\item
Analyze singular values of Jacobian of derivative array equations
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Iteration matrix condition number}
Consider
\begin{align*}
F(\dot x, x, y) &= 0 \\
G(x, y) &= 0
\end{align*}
\begin{itemize}
\item
BDF iteration matrix:
\begin{equation}
J = \begin{bmatrix}
\frac{\alpha_0}{h} \nabla_{\dot x}F + \nabla_x F & \nabla_y F \\
\nabla_x G & \nabla_y G
\end{bmatrix}
\end{equation}
\item
(Theorem 5.4.1) Condition number of iteration matrix is $\mathcal{O}(h^{-\nu})$
\item
Conjecture: If $\nabla_{\dot x}F = I$ and first row of $J$ is scaled by $h$, condition number is instead $\mathcal{O}(h^{-\nu-1})$
\begin{itemize}
\item
What to do if $\nabla_{\dot x}F$ is not square? What does IDA do?
\item
Maybe I should do something like this in my collocation implementation?
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Theorem and conjecture verification}
\begin{table}
\centering
\begin{tabular}{ll}
A: original problem & B: variable scaling \\
C: $a = \infty$ & s: conjecture scaling
\end{tabular}
\end{table}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{cond_test.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Condition number $t = 0$}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{cond_0.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Condition number $t = 6.21$}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{cond_621.pdf}
\end{figure}
\end{frame}



\begin{frame}
\frametitle{Derivative array equations}
Consider $F(x, \dot x) = 0$
\begin{itemize}
\item
Derivative array equations
\begin{align}
\bm F(x, \dot x, \ddot x) = \begin{bmatrix}
F(x, \dot x)\\
\frac{\mathrm{d}}{\mathrm{d}t} F(x, \dot x)
\end{bmatrix} = 0
\end{align}
\item
Proposition 2.5.1: Index is 1 if
$\displaystyle\frac{\partial \bm F(x, \dot x, \ddot x)}{\partial [\dot x, \ddot x]}$ is 1-full w.r.t. $\dot x$ and has constant rank
\item
Measure: $\displaystyle \frac{\sigma_1}{\sigma_k},$ where $\sigma_k$ is smallest singular value whose singular vector involves $\dot x$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Derivative array equations}
Problems with this measure:
\begin{itemize}
\item
Depends on problem scaling (good or bad?)
\item
Requires $\ddot x$ (could be obtained by adding states? might interfere?)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Next steps}
\begin{itemize}
\item
How to proceed?
\item
Claus mentioned stability issues before...
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Commutativity}
\begin{itemize}
\item
Previously: Does BLT and fixed-step collocation commute?
\item
Consider single integration step of
\begin{equation}
\begin{aligned}
\dot x &= f(x, y, u) \\
y &= g(x, u),
\end{aligned}
\end{equation}
and
\begin{equation}
\dot x = f(x, g(x, u), u)
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Commutativity}
Explicit DAE:
\begin{subequations}
\begin{gather}
\dot x_k = f(x_k, y_k, u_k), \\ 
y_k = g(x_k, u_k), \\
\dot{x}_k = \frac{1}{h} \cdot \sum_{n = 0}^{n_c} \alpha_{n, k} x_n, \\
\forall k \in [1 . . n_c]
\end{gather} 
\end{subequations}

Explicit ODE:
\begin{subequations}
\begin{gather}
\dot x_k = f(x_k, g(x_k, u_k), u_k), \\ 
\dot{x}_k = \frac{1}{h} \cdot \sum_{n = 0}^{n_c} \alpha_{n, k} x_n, \\
\forall k \in [1 . . n_c]
\end{gather} 
\end{subequations}

Clearly commutative?
\end{frame}

\begin{frame}
Bilevel control: Single-track used on high level to generate reference trajectory, double-track used on low level to follow trajectory
\frametitle{MPC}
\begin{columns}
\column{0.5\linewidth}
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{structure.pdf}
\end{figure}

\column{0.5\linewidth}
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{mpcxy.pdf}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Convergence}
\begin{itemize}
\item
High level needs to be solved over free time horizon $\implies$ difficult
\item
Low level has fixed time horizon (MPC) $\implies$ less difficult
\item
Newton's method does not converge at all for high-level problem if we use full DAE, but converges for ODE
\item
Usually converges for low level even with DAE, but ODE more robust and faster
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{High level}
Will today focus on high-level problem:
\begin{equation}
\begin{aligned}
	\underset{\delta_\mathrm{ref},\tau_{f,\mathrm{ref}},\tau_{r,\mathrm{ref}}}{\mathrm{minimize}} & \quad \int_{t_0}^{t_f}\!(\kappa_1 e^2 + \kappa_2\beta^2) \,\mathrm{d}t  \\
	\underset{\phantom{\delta_\mathrm{ref},\tau_{f,\mathrm{ref}},\tau_{r,\mathrm{ref}}}}{\mathrm{subject \ to}} & \quad    |\tau_{i,\mathrm{ref}}|
     \leq \tau_{i,\mathrm{max}}, \ \forall i \in \{f,r\}, \\[-10pt]
& \quad |\delta_\mathrm{ref}| \leq \delta_{\mathrm{max}},  \\
& \quad \lVert p(t_f)-p_{t_f}\rVert\leq \epsilon,\\
&           \quad \Gamma(p) \leq 0, \quad x(t_0) = x_0, \\
	& \quad F(\dot x,x,y,\delta_\mathrm{ref},\tau_{f,\mathrm{ref}},\tau_{r,\mathrm{ref}}) = 0
\end{aligned}
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Dynamic optimization}
More abstract/compact formulation ($\bm z := (\dot{\bm x}, \bm x, \bm y, \bm u)$):
{\small
\begin{subequations}\label{eq:DOP}
\begin{alignat}{3}
& \text{minimize \hspace{10pt}} && \int_{t_0}^{t_f} L(\bm z(t))\,\mathrm{d}t \label{eq:DOP!objective}, && \\
& \text{with respect to \hspace{10pt}} && \bm x : [t_0, t_f] \rightarrow \mathbb{R}^{n_x}, && \bm y : [t_0, t_f] \rightarrow \mathbb{R}^{n_y}, \nonumber \\
&&& \bm u : [t_0, t_f] \rightarrow \mathbb{R}^{n_u}, && t_f \in \mathbb{R}, \nonumber \\
& \text{subject to \hspace{10pt}} && \bm F(\bm z(t)) = \bm 0, &\quad& \bm F_0(\bm z(t_0)) = \bm 0, \label{eq:DOP!dynamics}\\
&&& \bm z_L \leq \bm z(t) \leq \bm z_U, \\
&&& \bm g(\bm z(t)) \leq \bm 0, &\quad& \bm G(\bm z(t_f)) \leq \bm 0, \label{eq:DOP!path_constraints} \\
&&& \forall t \in [t_0, t_f]. && \nonumber
\end{alignat}
\end{subequations}
}
\end{frame}

\begin{frame}
\frametitle{NLP}
{\small
Discretize differential equations to get a finite-dimensional nonlinear program (NLP) ($\bm z_{i, k} \approx \bm z(t_{i, k})$, where $t_{i, k}$ is collocation point $k$ in element $i$):
}
\vspace{-4pt}
{\small
\begin{subequations}
\begin{alignat}{3} 
& \text{minimize } && \rlap{$\displaystyle\sum_{i = 1}^{n_e} h_i \cdot (t_f - t_0) \cdot \sum_{k = 1}^{n_c} \omega_k \cdot L\left(\bm z_{i, k}\right)$,} \label{eq:DOP_NLP!objective} \\
& \text{with respect to \hspace{5pt}} && \rlap{$\bm z_{i, k} \in \mathbb{R}^{2n_x + n_y + n_u}, \quad \bm x_{i, 0} \in \mathbb{R}^{n_x}, \quad t_f \in \mathbb{R}$,} \nonumber \\  
& \text{subject to } && \bm F(\bm z_{i, k}) = \bm 0, &\quad& \bm F_0(\bm z_{1, 0}) = \bm 0 \label{eq:DOP_NLP!dynamics}, \\ 
&&& \bm u_{1, 0} = \sum_{k = 1}^{n_c} \bm u_{1, k} \cdot \ell_k(0) &\quad& \bm z_L \leq \bm z_{i, k} \leq \bm z_U, \label{eq:DOP_NLP!bounds} \\ 
&&& \bm g(\bm z_{i, k}) = \bm 0, &\quad& \bm G(\bm z_{n_e, n_c}) \leq \bm 0, \label{eq:DOP_NLP!point_constraints} \\
&&& \rlap{$\forall (i, k) \in \{(1, 0)\} \cup ([1 . . n_e] \times [1 . . n_c]),$} \nonumber \\ 
&&& \rlap{$\displaystyle\dot{\bm x}_{j, l} = \frac{1}{h_j \cdot (t_f - t_0)} \cdot \sum_{m = 0}^{n_c} \bm x_{j, m} \cdot \frac{\mathrm{d}\tilde{\ell}_m}{\mathrm{d}\tau}(\tau_l),$} \\ &&& \forall (j, l) \in [1 . . n_e] \times [1 . . n_c], \\ 
&&& \rlap{$\bm x_{n, n_c} = \bm x_{n+1, 0}, \quad \forall n \in [1..n_e-1].$} \label{eq:DOP_NLP!continuity} 
\end{alignat}
\end{subequations}
}
\end{frame}

\begin{frame}
\frametitle{NLP solution}
After further abstraction, the NLP is:
\begin{subequations}
    \begin{alignat*}{2}
        & \text{minimize } && f(x), \\
        & \text{with respect to \hspace{5pt}} && x \in \mathbb{R}^m, \\
        & \text{subject to } && x_L \leq x \leq x_U, \\
        &&& g(x) = 0, \\
        &&& h(x) \leq 0.
    \end{alignat*}
\end{subequations}
\begin{itemize}
\item
Solved by IPOPT
\item
Lots of complicated details, but essentially Newton's method is applied on KKT optimality conditions
\item
See bonus slides for details
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence}
Why convergence issues are more prominent in optimization than simulation in general:
\begin{itemize}
\item
Larger system of equations (dual variables + TBVP)
\item
No good initial guess (at least not for high-level problem)
\item
Inherently ill-conditioned (see bonus slides)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Restoration phase}
\begin{itemize}
\item
For the high-level problem, IPOPT fails in restoration
\item
Restoration is triggered for various reasons, but usually because of ill-conditioned Jacobian
\item
Restoration means that IPOPT stops solving the optimization problem and instead solves
\begin{subequations}
    \begin{alignat*}{2}
        & \text{minimize } && ||g(x)||_1 + ||h(x) - y||_1 + 0.5\zeta||D_R(x-x_R)||_2^2, \\
        & \text{with respect to \hspace{5pt}} && x \in \mathbb{R}^m, \quad y \in \mathbb{R}^n\\
        & \text{subject to } && x_L \leq x \leq x_U, \\
        &&& y \leq 0.
    \end{alignat*}
\end{subequations}
\item
IPOPT finds a local minimum to this problem which is not feasible; failure
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Next steps}
\begin{itemize}
\item
Analyzing IPOPT convergence issues is complicated
\begin{itemize}
\item
Next step I see in that direction is to compare $\text{cond}(\nabla_z F)$ for DAE and ODE formulation
\item
If significant difference, that is a likely explanation, but then we should try to figure out why there is a significant difference...
\end{itemize}
\item
Still don't understand why ODE is better for simulation
\begin{itemize}
\item
Probably not related to ill-conditioning?
\end{itemize}
\end{itemize}
\end{frame}

\end{document} 